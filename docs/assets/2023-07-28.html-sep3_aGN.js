import{_ as e,c as t,a as r,o as n}from"./app-YaGH6_b-.js";const l="/assets/langchain_chatglm-DaPhqfPv.png",o={};function p(s,a){return n(),t("div",null,a[0]||(a[0]=[r('<blockquote><p><strong>注意：</strong></p><p><strong>该博客内容不对真实性、专业性作出任何保障，仅作记录参考使用，如有错误欢迎联系指正</strong></p><p><strong>本调研内容截止日期为 2023 年 7 月 28 日，由于 LLM 的强时效性，您在进行参考时务必确认相关内容是否接近此日期及此日期之后的更新或修改</strong></p></blockquote><h2 id="引言" tabindex="-1"><a class="header-anchor" href="#引言"><span>引言</span></a></h2><p>近年来，大型语言模型（Large Language Model，LLM）取得了显著的进展。</p><p>其中，GPT-4 作为代表性的语言模型，在文本生成、问答系统和对话生成方面表现出色。然而，由于特定场景对隐私性和准确性的要求，需要寻找一种能够在本地部署大型语言模型并能提供准确回答的解决方案。</p><p>具体到本次调研，简单列出需求如下：</p><ol><li><p>在 24G 显存条件下可以完成项目部署；</p></li><li><p>可部署的模型对中文有一定支持；</p></li><li><p>项目提供给用户的答案应当是足够可靠的；</p></li><li><p>项目要便于部署、维护，有一定的社区生态支持。</p></li></ol><h2 id="llm-规模与-gpu-需求" tabindex="-1"><a class="header-anchor" href="#llm-规模与-gpu-需求"><span>LLM 规模与 GPU 需求</span></a></h2><p>通常，大型语言模型（Large Language Model，LLM）的规模会在模型名称中使用类似于<code>7B</code>的关键词，用来表示该模型的参数规模为 70 亿。一般性地，模型参数越多，模型效果越好，对硬件的需求也越高。</p><p>在 LLM 的部署过程中，可以采用模型量化技术对模型参数进行压缩，以降低硬件需求、提升计算速度。常见的量化等级包括<code>FP16</code>、<code>BF16</code>、<code>INT8</code>和<code>INT4</code>，分别代表使用 16 位浮点数、16 位浮点数（Google Brain Floating）、8 位整数和 4 位整数进行近似计算。据不可靠来源，对模型进行量化可能会使模型更容易出现幻觉，因此本次部署中在相似条件下优先选择<code>FP16</code>、<code>BF16</code>精度进行部署。</p><p>以<a href="https://huggingface.co/THUDM/chatglm2-6b" target="_blank" rel="noopener noreferrer">ChatGLM2-6B</a>为例，在不同的量化等级下所需的最小显存如下：</p><table><thead><tr><th><strong>量化等级</strong></th><th><strong>编码 2048 长度的最小显存</strong></th><th><strong>生成 8192 长度的最小显存</strong></th></tr></thead><tbody><tr><td>FP16 / BF16</td><td>13.1 GB</td><td>12.8 GB</td></tr><tr><td>INT8</td><td>8.2 GB</td><td>8.1 GB</td></tr><tr><td>INT4</td><td>5.5 GB</td><td>5.1 GB</td></tr></tbody></table><p>一般而言，<code>7B</code>规模的模型以<code>INT4</code>量化需要 5G 显存，在此基础上参数规模每提高<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>倍，需要的显存大小变为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">2^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6644em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span>。如果以 16 位浮点数精度部署模型，24G 显存能运行的最大模型为 14B 左右。</p><h3 id="量化对模型性能的影响" tabindex="-1"><a class="header-anchor" href="#量化对模型性能的影响"><span><strong>量化对模型性能的影响</strong></span></a></h3><p>GLM 团队也测试了<a href="https://github.com/THUDM/ChatGLM2-6B#%E6%8E%A8%E7%90%86%E6%80%A7%E8%83%BD" target="_blank" rel="noopener noreferrer">量化对模型性能的影响</a>。结果表明，量化对模型性能的影响在可接受范围内。</p><table><thead><tr><th>ChatGLM-6B 量化等级</th><th>Accuracy (<a href="https://github.com/hendrycks/test" target="_blank" rel="noopener noreferrer">MMLU</a>)</th><th>Accuracy (<a href="https://cevalbenchmark.com/static/leaderboard.html" target="_blank" rel="noopener noreferrer">C-Eval dev</a>)</th></tr></thead><tbody><tr><td>BF16</td><td>45.47</td><td>53.57</td></tr><tr><td>INT4</td><td>43.13</td><td>50.30</td></tr></tbody></table><h2 id="模型选取" tabindex="-1"><a class="header-anchor" href="#模型选取"><span>模型选取</span></a></h2><p>在模型选取阶段，我们参考了一些国内外各模型测评方案、LLM 跟踪榜单，主要包括以下内容：</p><ol><li><strong>评测集</strong></li></ol><ul><li><p><a href="https://github.com/hendrycks/test" target="_blank" rel="noopener noreferrer">MMLU</a>：一个包含 57 个多选任务的英文评测数据集，涵盖了初等数学、美国历史、计算机科学、法律等，难度覆盖高中水平到专家水平，在<a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">Hugging Face</a>上提供了<a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" target="_blank" rel="noopener noreferrer">Open LLM Leaderboard</a>测试信息统计。</p></li><li><p><a href="https://github.com/SJTU-LIT/ceval" target="_blank" rel="noopener noreferrer">C-Eval</a>：一个全面的中文基础模型评估套件，涵盖了 52 个不同学科的 13948 个多项选择题，在<a href="https://cevalbenchmark.com/index_zh.html" target="_blank" rel="noopener noreferrer">官网</a>提供了<a href="https://cevalbenchmark.com/static/leaderboard_zh.html" target="_blank" rel="noopener noreferrer">C-Eval 排行榜</a>。</p></li><li><p><a href="https://github.com/OpenLMLab/GAOKAO-Bench" target="_blank" rel="noopener noreferrer">Gaokao</a>：由复旦大学研究团队构建的基于中国高考题目的综合性考试评测集，包含多科目多题型，并提供了 9 种模型的评测结果。</p></li><li><p><a href="https://arxiv.org/pdf/2304.06364.pdf" target="_blank" rel="noopener noreferrer">AGIEval</a>：用于评估基础模型在与人类认知和解决问题相关的任务中的一般能力，基于普通大学入学考试、数学竞赛、律师资格考试等 20 项面向普通考生的官方、公开、高标准的入学和资格考试构建。</p></li><li><p><a href="https://github.com/haonan-li/CMMLU" target="_blank" rel="noopener noreferrer">CMMLU</a>：一个综合性的中文评估基准，专门用于评估语言模型在中文语境下的知识和推理能力，涵盖了从基础学科到高级专业水平的 67 个主题，提供了 14 种 LLM 的评测结果。</p></li><li><p><a href="https://github.com/michael-wzhu/PromptCBLUE" target="_blank" rel="noopener noreferrer">PromptCBLUE</a>：将 16 种不同的医疗场景 NLP 任务转化为基于提示的语言生成任务形成的首个中文医疗场景的 LLM 评测基准，当前在<a href="https://tianchi.aliyun.com/competition/entrance/532084/introduction" target="_blank" rel="noopener noreferrer">阿里巴巴天池大赛平台</a>上线进行开放评测。</p></li></ul><ol start="2"><li><strong>LLM 跟踪榜单</strong></li></ol><ul><li><p><a href="https://github.com/HqWu-HITCS/Awesome-Chinese-LLM" target="_blank" rel="noopener noreferrer">An Awesome Collection for LLM in Chinese</a>：已收集超过 100 个中文 LLM 相关的开源模型、应用、数据集及教程。</p></li><li><p><a href="https://github.com/jeinlee1991/chinese-llm-benchmark" target="_blank" rel="noopener noreferrer">CLiB 中文大模型能力评测榜单（持续更新）</a>:提供分类能力、信息抽取能力、阅读理解能力、表格问答能力等多个维度的大模型评测结果。</p></li><li><p><a href="https://github.com/chenking2020/FindTheChatGPTer" target="_blank" rel="noopener noreferrer">寻找那些 ChatGPT/GPT4 开源“平替”们</a>：ChatGPT/GPT4 开源“平替”汇总，包括自主模型、Alpaca 模式微调模型、AGI 项目、榜单、语料等多维度内容。</p></li></ul><h2 id="相关开源项目对比" tabindex="-1"><a class="header-anchor" href="#相关开源项目对比"><span>相关开源项目对比</span></a></h2><p>基于以上调研，项目的选取范围定为：支持本地知识库的 LLM 调用项目。</p><p>初步选取符合要求的项目有三个：<a href="https://github.com/chatchat-space/langchain-ChatGLM" target="_blank" rel="noopener noreferrer">langchain-ChatGLM：基于本地知识库的 ChatGLM 等大语言模型应用实现</a>、<a href="https://github.com/wenda-LLM/wenda" target="_blank" rel="noopener noreferrer">闻达：一个大规模语言模型调用平台</a>、<a href="https://github.com/binary-husky/gpt_academic" target="_blank" rel="noopener noreferrer">gpt_academic：GPT 学术优化</a>。</p><p>模型方面，当前最优 LLM 模型为<a href="https://github.com/THUDM/ChatGLM2-6B" target="_blank" rel="noopener noreferrer"><code>ChatGLM2-6B</code></a>，知识库向量化模型选择<a href="https://huggingface.co/moka-ai/m3e-base" target="_blank" rel="noopener noreferrer"><code>m3e-base</code></a>。</p><h3 id="langchain-chatglm" tabindex="-1"><a class="header-anchor" href="#langchain-chatglm"><span><strong>langchain-ChatGLM</strong></span></a></h3><p>langchain-ChatGLM 是一个利用<a href="https://github.com/hwchase17/langchain" target="_blank" rel="noopener noreferrer">langchain</a>思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案。</p><figure><img src="'+l+'" alt="langchain-ChatGLM" tabindex="0" loading="lazy" width="1206" height="798"><figcaption>langchain-ChatGLM</figcaption></figure><p>该项目主要特点包括：</p><ul><li><p>支持 Docker 部署，同时提供开发部署方案</p></li><li><p>支持 ChatGLM、ChatYuan 等多种模型，支持通过 fastchat API 调用任意 LLM</p></li><li><p>知识库接入非结构化文档，当前已支持 md、pdf、docx、txt 格式，支持 jpg 与 png 格式图片的 OCR 文字识别</p></li><li><p>路线图（未来计划实现的功能）中包含结构化数据（csv、Excel、SQL 等）接入、知识图谱/图数据库接入、Agent 实现</p></li><li><p>支持多种知识库 Embedding 模型</p></li><li><p>支持搜索引擎问答</p></li></ul><h3 id="闻达" tabindex="-1"><a class="header-anchor" href="#闻达"><span><strong>闻达</strong></span></a></h3><p>闻达项目的设计目标是实现针对特定环境的高效内容生成，同时考虑个人和中小企业的计算资源局限性，以及知识安全和私密性问题。该项目的主要特点包括：</p><ul><li><p>支持 chatGLM-6B\\chatGLM2-6B、chatRWKV、llama 系列、openai api 等多种模型接入</p></li><li><p>支持本地离线向量库（rtst）、本地搜索引擎（fess）、在线搜索引擎三种知识库构建模式</p></li><li><p>特色 Auto 脚本：通过开发插件形式的 JavaScript 脚本，为平台附件功能，实现包括但不限于自定义对话流程、访问外部 API、在线切换 LoRA 模型</p></li><li><p>使用宏调用 API 的方式接入 Word 文档</p></li></ul><p>值得一提的是，由社区成员 AlanLee1996 贡献的<a href="https://github.com/AlanLee1996/wenda-webui" target="_blank" rel="noopener noreferrer">Wenda-Webui</a>提供了类似 ChatPDF 的文档对话功能。</p><h3 id="gpt-academic" tabindex="-1"><a class="header-anchor" href="#gpt-academic"><span><strong>gpt_academic</strong></span></a></h3><p>gpt_academic 是为 ChatGPT/GLM 设计的图形交互界面，特别优化论文阅读/润色/写作体验，模块化设计，主要特点包括：</p><ul><li><p>支持复旦 MOSS、llama、rwkv、newbing、claude、claude2 等多种模型接入</p></li><li><p>支持多种模型混合调用，支持模型异步加载</p></li><li><p>支持 latex 格式论文翻译、总结、润色，支持 latex 公式渲染</p></li><li><p>支持自定义强大的函数插件，插件支持热更新</p></li><li><p>丰富的插件库，支持 PDF latex 论文解析、代码工程解释、批量注释生成等多种功能</p></li></ul>',37)]))}const i=e(o,[["render",p]]),c=JSON.parse('{"path":"/%E5%8D%9A%E5%AE%A2/2023-07-28.html","title":"开源LLM模型及社区项目调研","lang":"zh-CN","frontmatter":{"title":"开源LLM模型及社区项目调研","createTime":"2023/07/28 20:50:46","author":"yuany3721 danc","tags":["LLM"],"categories":["blog"]},"headers":[],"git":{"updatedTime":1742715324000,"contributors":[{"name":"yuany3721","username":"yuany3721","email":"li1116@mail.ustc.edu.cn","commits":1,"avatar":"https://avatars.githubusercontent.com/yuany3721?v=4","url":"https://github.com/yuany3721"}]},"filePathRelative":"博客/2023-07-28.md","categoryList":[{"id":"c50d13","sort":10000,"name":"博客"}]}');export{i as comp,c as data};
